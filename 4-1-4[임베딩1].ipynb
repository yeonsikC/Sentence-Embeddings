{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. NPLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예측 기반 모델\n",
    "- 기존 언어 모델의 한계를 일부 극복\n",
    ">학습 데이터에 존재하지 않는 n-gram이 포함된 문장이 나타낼 확률 값을 0으로 부여<br>\n",
    "단어/문장 간 유사도를 계산할 수 없음\n",
    "\n",
    "- 단어 시퀀스가 주어졌을 때 다음 단어가 무엇인지 맞추는 과정에서 학습됨\n",
    ">입력 : n개의 단어 인덱스 값을 확인하고 각 단어에 해당하는 열 벡터를 C(어휘집합크기x차원 수 행렬)에서 참조한 뒤, 하나로 묶어줌<br>\n",
    "학습 : 정답 단어의 인덱스와 비교해 역전파하는 방식으로 학습<br>\n",
    "학습이 종료되면 행렬 C를 각 단어에 해당하는 m차원 임베딩으로 사용\n",
    "\n",
    "- NPLM은 학습해야 하는 파라미터 종류가 많고 크기가 큰 편 -> 학습속도가 느리다!\n",
    "- but, 문장이 말뭉치에 없어도 문맥이 비슷한 다른 문장을 참고해 확률을 부여한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 구글 연구팀이 발표한 가장 널리 쓰이고 있는 단어 임베딩 모델\n",
    "- CBOW와 Skip-gram 두 가지 모델이 제안되었으며, 네거티브 샘플링, 서브 샘플링 등 학습 최적화 기법이 제안됨\n",
    "> CBOW : 주변에 있는 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습<br>\n",
    "Skip-gram : 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측<br>\n",
    "일반적으로 Skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 더 좋음\n",
    "\n",
    "- 네거티브 샘플링 : 타깃 단어와 문맥 단어 쌍이 주어졌을 때 해당 쌍이 포지티브 샘플인지, 네거티브 샘플인지 이진분류하는 과정에서 학습되는 기법<br>\n",
    ">포지티브 샘플 : 타깃 단어와 그 주변에 실제로 등장한 문맥 단어 쌍<br>\n",
    "네거티브 샘플 : 타깃 딘아어와 그 주변에 등장하지 않은 단어 쌍\n",
    "\n",
    "- 서브샘플링 : 자주 등장하는 단어는 학습에서 제외\n",
    "- Skip-gram 모델은 모델 파라미터를 한 번 업데이트 할 때, 1개 쌍의 포지티브 샘플과 k개의 네거티브 샘플을 학습함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git pull origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/embedding\n"
     ]
    }
   ],
   "source": [
    "#cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!bash preprocess.sh dump-tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ../data/tokenized/wiki_ko_mecab.txt ../data/tokenized/ratings_mecab.txt ../data/tokenized/korquad_mecab.txt > ../data/tokenized/corpus_mecab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fname = \"/notebooks/embedding/data/tokenized/corpus_mecab.txt\"\n",
    "model_fname = \"/notebooks/embedding/data/word-embeddings/word2vec/word2vec\"\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "corpus = [sent.strip().split(\" \") for sent in open(corpus_fname, \"r\").readlines()]\n",
    "model = Word2Vec(corpus, size=100, workers=4, sg=1)\n",
    "#size : Word2Vec 임베딩 차원 수\n",
    "#workers : CPU 스레드 개수\n",
    "#sg : skip-gram 여부 -> 0이면 CBOW로 학습\n",
    "model.save(model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/embedding\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('소망', 0.7979258),\n",
       " ('꿈', 0.7846539),\n",
       " ('행복', 0.76131797),\n",
       " ('희망찬', 0.7413486),\n",
       " ('땀방울', 0.7262753)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.word_eval import WordEmbeddingEvaluator\n",
    "model = WordEmbeddingEvaluator(\"/notebooks/embedding/data/word-embeddings/word2vec/word2vec\", method=\"word2vec\", dim=100, tokenizer_name=\"mecab\")\n",
    "model.most_similar(\"희망\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FastText는 페이스북에서 개발해 공개한 단어 임베딩 기법\n",
    "- 각 단어를 문자 단위 n-gram으로 표현하며, 네거티브 샘플링 기법을 사용\n",
    "- 모델을 한 번 업데이트할 때 1개의 포지티브 샘플과 k개의 네거티브 샘플을 학습\n",
    "- 오타나 미등록 단어에 강함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 233M words\n",
      "Number of words:  358043\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  106732 lr:  0.000000 avg.loss:  0.471360 ETA:   0h 0m 0s 1.751751 ETA:   0h16m42s avg.loss:  1.490483 ETA:   0h15m34s 1.471551 ETA:   0h15m27s avg.loss:  1.469844 ETA:   0h15m23s 1.421293 ETA:   0h14m53s 1.415598 ETA:   0h14m52s ETA:   0h14m49s ETA:   0h14m36s ETA:   0h14m31s 1.379386 ETA:   0h14m20s 1.378151 ETA:   0h14m18s 1.375239 ETA:   0h14m15s  0h14m11s 1.355077 ETA:   0h14m 9s 1.342321 ETA:   0h14m 5s 4s  0h13m55s 1.293545 ETA:   0h13m51s 1.269830 ETA:   0h13m44s 1.261470 ETA:   0h13m41s avg.loss:  1.253478 ETA:   0h13m38s 1.249585 ETA:   0h13m36s words/sec/thread:  104731 lr:  0.043202 avg.loss:  1.221544 ETA:   0h13m23s avg.loss:  1.216588 ETA:   0h13m21s  0h13m19s 1.211285 ETA:   0h13m17s 1.164694 ETA:   0h12m31s 1.158253 ETA:   0h12m24s avg.loss:  1.110803 ETA:   0h12m 6s lr:  0.039228 avg.loss:  1.103464 ETA:   0h12m 4s 1.096447 ETA:   0h12m 2s ETA:   0h11m57s 1.056588 ETA:   0h11m51s 1.010707 ETA:   0h11m36s 1.005384 ETA:   0h11m34s avg.loss:  1.000184 ETA:   0h11m31s 0.974030 ETA:   0h11m22s 0.945236 ETA:   0h11m11s 0.936068 ETA:   0h11m 7s 0.918729 ETA:   0h10m59s 0.910457 ETA:   0h10m55s 0.891028 ETA:   0h10m44s 0.880415 ETA:   0h10m38s 0.873427 ETA:   0h10m34s avg.loss:  0.869874 ETA:   0h10m32s 0.862983 ETA:   0h10m28s ETA:   0h10m10s  0h10m 7s 0.825918 ETA:   0h10m 5s 105843 lr:  0.032425 avg.loss:  0.812279 ETA:   0h 9m56s 0.031837 avg.loss:  0.797498 ETA:   0h 9m45s 105831 lr:  0.031599 avg.loss:  0.791633 ETA:   0h 9m41s 0.031485 avg.loss:  0.789114 ETA:   0h 9m39s lr:  0.031425 avg.loss:  0.787798 ETA:   0h 9m38s 105851 lr:  0.030756 avg.loss:  0.772794 ETA:   0h 9m25s lr:  0.030339 avg.loss:  0.763749 ETA:   0h 9m18s lr:  0.030224 avg.loss:  0.761231 ETA:   0h 9m16s 0.029883 avg.loss:  0.753931 ETA:   0h 9m 9s% words/sec/thread:  105900 lr:  0.029052 avg.loss:  0.737035 ETA:   0h 8m54s 105906 lr:  0.028931 avg.loss:  0.734856 ETA:   0h 8m52s words/sec/thread:  105950 lr:  0.028256 avg.loss:  0.722860 ETA:   0h 8m39s words/sec/thread:  105958 lr:  0.028134 avg.loss:  0.720774 ETA:   0h 8m37s% words/sec/thread:  105960 lr:  0.027943 avg.loss:  0.717509 ETA:   0h 8m33s words/sec/thread:  106028 lr:  0.027312 avg.loss:  0.706519 ETA:   0h 8m21s words/sec/thread:  106089 lr:  0.026239 avg.loss:  0.690582 ETA:   0h 8m 1s 0.684954 ETA:   0h 7m54s 0.683093 ETA:   0h 7m52sh 7m50s 0.024481 avg.loss:  0.665782 ETA:   0h 7m28s 106235 lr:  0.024121 avg.loss:  0.661266 ETA:   0h 7m22s avg.loss:  0.655471 ETA:   0h 7m14s  0h 7m12s59s ETA:   0h 6m49s 5m30s 5m28s ETA:   0h 5m19s 0.591087 ETA:   0h 5m17s avg.loss:  0.584361 ETA:   0h 5m 3s 0.016166 avg.loss:  0.580784 ETA:   0h 4m55s lr:  0.015716 avg.loss:  0.577132 ETA:   0h 4m47s 0.575561 ETA:   0h 4m43s 0.015041 avg.loss:  0.572354 ETA:   0h 4m35s ETA:   0h 4m31s 0.566286 ETA:   0h 4m19s avg.loss:  0.560535 ETA:   0h 4m 3s 0.558708 ETA:   0h 3m57s 0.558194 ETA:   0h 3m56s ETA:   0h 3m54s 0.556939 ETA:   0h 3m52s 0.012606 avg.loss:  0.556325 ETA:   0h 3m50s avg.loss:  0.555522 ETA:   0h 3m48s lr:  0.012274 avg.loss:  0.554298 ETA:   0h 3m44s 0.553121 ETA:   0h 3m40s 0.011948 avg.loss:  0.552588 ETA:   0h 3m38s 0.551991 ETA:   0h 3m36s 0.551354 ETA:   0h 3m34s ETA:   0h 3m31s lr:  0.011456 avg.loss:  0.549745 ETA:   0h 3m29s 0.545476 ETA:   0h 3m15s 0.542743 ETA:   0h 3m 5s ETA:   0h 2m59s 0.535809 ETA:   0h 2m41s ETA:   0h 2m39s 0.533084 ETA:   0h 2m31s 0.532181 ETA:   0h 2m28s 0.531605 ETA:   0h 2m25s avg.loss:  0.529904 ETA:   0h 2m19s 0.529596 ETA:   0h 2m18s ETA:   0h 2m11s 0.526770 ETA:   0h 2m 7s 106691 lr:  0.006889 avg.loss:  0.526255 ETA:   0h 2m 5s ETA:   0h 2m 1s ETA:   0h 1m59s ETA:   0h 1m55s 0.520126 ETA:   0h 1m43s 0.519090 ETA:   0h 1m39s ETA:   0h 1m37s35s 0.517441 ETA:   0h 1m33s avg.loss:  0.514041 ETA:   0h 1m20sh 1m18s 0.513073 ETA:   0h 1m16s avg.loss:  0.512754 ETA:   0h 1m15s 0.003905 avg.loss:  0.510390 ETA:   0h 1m11s ETA:   0h 1m 7s 0.498721 ETA:   0h 0m51s 0.493141 ETA:   0h 0m41s avg.loss:  0.490990 ETA:   0h 0m37s lr:  0.001936 avg.loss:  0.489899 ETA:   0h 0m35s 0.482340 ETA:   0h 0m21s ETA:   0h 0m19s ETA:   0h 0m13s 0.475135 ETA:   0h 0m 7s 0.473317 ETA:   0h 0m 3s\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data/word-embeddings/fasttext\n",
    "!models/fastText/fasttext skipgram -input data/tokenized/corpus_mecab.txt -output data/word-embeddings/fasttext/fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('행복', 0.7580105238756496),\n",
       " ('소망', 0.7127900391029476),\n",
       " ('희망찬', 0.6947392072017382),\n",
       " ('기쁨', 0.6851204947805614),\n",
       " ('꿈', 0.6769203079901275)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.word_eval import WordEmbeddingEvaluator\n",
    "model = WordEmbeddingEvaluator(vecs_txt_fname=\"data/word-embeddings/fasttext/fasttext.vec\",\n",
    "                              vecs_bin_fname=\"data/word-embeddings/fasttext/fasttext.bin\",\n",
    "                               method=\"fasttext\", dim=100, tokenizer_name=\"mecab\")\n",
    "model.most_similar(\"희망\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fastText는 문자 단위 n-gram을 쓰기 때문에 한글과 궁합이 잘 맞음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**자소 단위로 분해한 데이터로 FastText 시행**<br>\n",
    "- 은전한닢으로 형태소 분석을 시행\n",
    "- 이를 자소단위로 분해한 데이터를 만들기\n",
    "- 이 데이터를 이용해 FastText 임베딩 시행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㄴㅏ- ㄴㅡㄴ ㅎㅏㄱㄱㅛ- ㅇㅔ- ㄱㅏㄴㄷㅏ-\n"
     ]
    }
   ],
   "source": [
    "from preprocess import jamo_sentence, get_tokenizer\n",
    "tokenizer = get_tokenizer(\"mecab\")\n",
    "tokens = \" \".join(tokenizer.morphs(\"나는 학교에 간다\"))\n",
    "print(jamo_sentence(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess/unsupervised_nlputils.py --preprocess_mode jamo \\\n",
    "        --input_path /notebooks/embedding/data/tokenized/corpus_mecab.txt \\\n",
    "        --output_path /notebooks/embedding/data/tokenized/corpus_mecab_jamo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 233M words\n",
      "Number of words:  358043\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   81757 lr:  0.000000 avg.loss:  0.474656 ETA:   0h 0m 0s lr:  0.049927 avg.loss:  1.500141 ETA:   0h21m39s 1.340330 ETA:   0h20m18s 1.396855 ETA:   0h19m58s avg.loss:  1.392696 ETA:   0h19m42s 1.388946 ETA:   0h19m36s words/sec/thread:   80311 lr:  0.048291 avg.loss:  1.386808 ETA:   0h19m31s words/sec/thread:   80419 lr:  0.048214 avg.loss:  1.386763 ETA:   0h19m27s  80456 lr:  0.048131 avg.loss:  1.386841 ETA:   0h19m25s% words/sec/thread:   80843 lr:  0.047610 avg.loss:  1.375748 ETA:   0h19m 6s words/sec/thread:   80814 lr:  0.047456 avg.loss:  1.371540 ETA:   0h19m 3s% words/sec/thread:   80865 lr:  0.047380 avg.loss:  1.364617 ETA:   0h19m 1s% words/sec/thread:   80917 lr:  0.046966 avg.loss:  1.342125 ETA:   0h18m50s% words/sec/thread:   80904 lr:  0.046795 avg.loss:  1.340162 ETA:   0h18m46s words/sec/thread:   80867 lr:  0.046459 avg.loss:  1.335919 ETA:   0h18m38s  80870 lr:  0.046376 avg.loss:  1.335428 ETA:   0h18m36s  8.2% words/sec/thread:   80993 lr:  0.045903 avg.loss:  1.334496 ETA:   0h18m23s% words/sec/thread:   81032 lr:  0.045734 avg.loss:  1.332487 ETA:   0h18m19s% words/sec/thread:   81058 lr:  0.045098 avg.loss:  1.306731 ETA:   0h18m 3s% words/sec/thread:   81480 lr:  0.044510 avg.loss:  1.262533 ETA:   0h17m43s% words/sec/thread:   81550 lr:  0.044413 avg.loss:  1.254695 ETA:   0h17m40s% words/sec/thread:   81550 lr:  0.044153 avg.loss:  1.239811 ETA:   0h17m34s% words/sec/thread:   81502 lr:  0.043854 avg.loss:  1.227301 ETA:   0h17m27s words/sec/thread:   81446 lr:  0.043741 avg.loss:  1.222993 ETA:   0h17m25s words/sec/thread:   81424 lr:  0.043621 avg.loss:  1.217652 ETA:   0h17m23s% words/sec/thread:   81425 lr:  0.043503 avg.loss:  1.211941 ETA:   0h17m20s words/sec/thread:   81386 lr:  0.042890 avg.loss:  1.189259 ETA:   0h17m 6s% words/sec/thread:   81389 lr:  0.042793 avg.loss:  1.187223 ETA:   0h17m 3s% words/sec/thread:   81399 lr:  0.042692 avg.loss:  1.184754 ETA:   0h17m 1s words/sec/thread:   81380 lr:  0.042333 avg.loss:  1.176588 ETA:   0h16m53s words/sec/thread:   81394 lr:  0.042235 avg.loss:  1.173507 ETA:   0h16m50s words/sec/thread:   81417 lr:  0.042137 avg.loss:  1.170868 ETA:   0h16m47s% words/sec/thread:   81429 lr:  0.042039 avg.loss:  1.168815 ETA:   0h16m45s 1.162015 ETA:   0h16m38s 1.154887 ETA:   0h16m30s ETA:   0h16m26s 1.143576 ETA:   0h16m13s avg.loss:  1.137279 ETA:   0h16m 1s 1.129843 ETA:   0h15m50s ETA:   0h15m40s 1.065660 ETA:   0h15m20s  81625 lr:  0.038252 avg.loss:  1.045716 ETA:   0h15m12s  81660 lr:  0.038088 avg.loss:  1.036297 ETA:   0h15m 8s  81674 lr:  0.037921 avg.loss:  1.026376 ETA:   0h15m 4s 1.016584 ETA:   0h14m59s 1.012328 ETA:   0h14m57s 0.037574 avg.loss:  1.007922 ETA:   0h14m55s% words/sec/thread:   81753 lr:  0.037324 avg.loss:  0.995296 ETA:   0h14m49s  81751 lr:  0.037160 avg.loss:  0.987285 ETA:   0h14m45s lr:  0.037079 avg.loss:  0.983409 ETA:   0h14m43s 0.036925 avg.loss:  0.975736 ETA:   0h14m39s lr:  0.036761 avg.loss:  0.967745 ETA:   0h14m36s32s lr:  0.035784 avg.loss:  0.923956 ETA:   0h14m14s lr:  0.035697 avg.loss:  0.920371 ETA:   0h14m12s  0h14m 6s ETA:   0h13m56s13m46ss35s 0.864043 ETA:   0h13m33s 0.840344 ETA:   0h13m15s13m 5s  0h13m 0sss  0h12m45s 0.776257 ETA:   0h12m16s avg.loss:  0.771238 ETA:   0h12m11s46s lr:  0.028933 avg.loss:  0.735050 ETA:   0h11m31s lr:  0.028597 avg.loss:  0.728838 ETA:   0h11m23s 6s  0h10m22s avg.loss:  0.685954 ETA:   0h10m19s 0.025777 avg.loss:  0.683208 ETA:   0h10m15s 0.025544 avg.loss:  0.679768 ETA:   0h10m 9s lr:  0.025191 avg.loss:  0.674861 ETA:   0h10m 1sh 9m56ss  0h 9m32s15s 9m10ss 8m 1s  81648 lr:  0.019932 avg.loss:  0.613380 ETA:   0h 7m55s 0.611000 ETA:   0h 7m49s avg.loss:  0.609513 ETA:   0h 7m45s 7m42s ETA:   0h 7m40s avg.loss:  0.605138 ETA:   0h 7m34s  0h 7m23s% words/sec/thread:   81619 lr:  0.018421 avg.loss:  0.599553 ETA:   0h 7m19s avg.loss:  0.594355 ETA:   0h 7m 5s 0.017513 avg.loss:  0.591685 ETA:   0h 6m57s 0.590265 ETA:   0h 6m54s 0.017281 avg.loss:  0.589582 ETA:   0h 6m52s 0.585292 ETA:   0h 6m39s 0.016526 avg.loss:  0.583565 ETA:   0h 6m34s 0.582866 ETA:   0h 6m32s avg.loss:  0.582110 ETA:   0h 6m30s 0.016219 avg.loss:  0.580743 ETA:   0h 6m26s 0.016147 avg.loss:  0.580128 ETA:   0h 6m25s 0.579417 ETA:   0h 6m23s 0.015775 avg.loss:  0.576879 ETA:   0h 6m16s 0.576546 ETA:   0h 6m15s avg.loss:  0.576056 ETA:   0h 6m13s 0.573719 ETA:   0h 6m 6s 0.572147 ETA:   0h 6m 0s 0.570497 ETA:   0h 5m55s 0.570013 ETA:   0h 5m53s lr:  0.014524 avg.loss:  0.568045 ETA:   0h 5m46s 0.566517 ETA:   0h 5m40s lr:  0.013754 avg.loss:  0.562970 ETA:   0h 5m28s 0.562448 ETA:   0h 5m26s 0.560588 ETA:   0h 5m19s  0h 5m 2sm57ss 0.545884 ETA:   0h 4m19s avg.loss:  0.544984 ETA:   0h 4m16s ETA:   0h 4m10s 0.538229 ETA:   0h 3m44s 0.537838 ETA:   0h 3m43s 0.537068 ETA:   0h 3m39s 0.535789 ETA:   0h 3m34s 0.533625 ETA:   0h 3m23s lr:  0.008458 avg.loss:  0.533291 ETA:   0h 3m21s 0.531075 ETA:   0h 3m10s 0.007317 avg.loss:  0.527954 ETA:   0h 2m54s 0.007168 avg.loss:  0.527319 ETA:   0h 2m50s avg.loss:  0.525480 ETA:   0h 2m41s 0.524835 ETA:   0h 2m38s  81777 lr:  0.006424 avg.loss:  0.523865 ETA:   0h 2m32s 0.522718 ETA:   0h 2m27s 0.006060 avg.loss:  0.521973 ETA:   0h 2m24s 0.521241 ETA:   0h 2m20s17s12s58ss  0h 1m50s47s  0h 1m46ss 0.513259 ETA:   0h 1m39sm36ss avg.loss:  0.511476 ETA:   0h 1m30s 0.511160 ETA:   0h 1m28ss17ss14ss 0.493945 ETA:   0h 0m47s44s 0.490405 ETA:   0h 0m39s 0.001502 avg.loss:  0.488931 ETA:   0h 0m35s 0.486777 ETA:   0h 0m30s 0.484672 ETA:   0h 0m25s 0.482820 ETA:   0h 0m20s 0.480963 ETA:   0h 0m16s avg.loss:  0.480313 ETA:   0h 0m14s\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data/word-embeddings/fasttext-jamo\n",
    "!models/fastText/fasttext skipgram -input data/tokenized/corpus_mecab_jamo.txt -output data/word-embeddings/fasttext-jamo/fasttext-jamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/embedding'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('희망찬', 0.8165260621686699),\n",
       " ('행복', 0.7887172253399581),\n",
       " ('희망특강', 0.7397415223317794),\n",
       " ('소망', 0.7135493994970037),\n",
       " ('희망자', 0.7108970999198214)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.word_eval import WordEmbeddingEvaluator\n",
    "model = WordEmbeddingEvaluator(vecs_txt_fname=\"data/word-embeddings/fasttext-jamo/fasttext-jamo.vec\",\n",
    "                              vecs_bin_fname=\"data/word-embeddings/fasttext-jamo/fasttext-jamo.bin\",\n",
    "                               method=\"fasttext-jamo\", dim=100, tokenizer_name=\"mecab\")\n",
    "model.most_similar(\"희망\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('서울', 0.9475430095956829),\n",
       " ('서울특별시', 0.8207505619858799),\n",
       " ('특별시세', 0.8071229614705601),\n",
       " ('서초구', 0.7877031110761973),\n",
       " ('서울시', 0.7874081173655547)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#미등록 단어에 대한 자소 단위 FastText임베딩 결과\n",
    "model._is_in_vocabulary(\"서울특별시\")\n",
    "model.get_word_vector(\"서울특별시\")\n",
    "model.most_similar(\"서울특별시\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. 잠재 의미 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 커다란 행렬에 차원 축소 방법의 일종인 특이값 분해를 수행하는 기법\n",
    "- 계산의 효율성을 키우는 한편 행간에 숨어 있는 잠재 의미를 이끌어내기 위한 방법론\n",
    "- 특이값 분해를 시행한 뒤 결과로 도출되는 행 벡터들을 단어 임베딩으로 상요할 수 있음\n",
    "- 단어-문서 행렬, TF-IDF 행렬, 단어-문맥 행렬, 점별 상호 정보량(PMI)에 모두 잠재 의미 분석을 수행할 수 있음\n",
    "- 자연어처리 분야에서는 PMI 대신 PPMI를 사용할 수 있음\n",
    ">PMI : 두 확률변수 사이의 상관성을 계량화한 지표<br>\n",
    "PPMI : PMI가 양수가 아닐 경우 그 값을 신뢰하기 어려워 0으로 치환해 무시<br>\n",
    "Shifted PMI : PMI에서 logk를 빼준 값으로, Word2Vec과 깊은 연관이 있음\n",
    "\n",
    "- 잠재 의미 분석에 행렬 분해를 이용할 수 있음\n",
    ">truncated SVD : 특이값 가운데 가장 큰 d개만 가지고 해당 특이값에 대응하는 특이 벡터들로 원래 행렬을 근사하는 방ㅂ버\n",
    "\n",
    "- LSA를 이용하면 단어와 문맥 간의 내재적인 의미를 효과적으로 보존할 수 있게 돼 문서 간 유사도 측정 등 모델의 성능 향상에 도움을 줄 수 있다고 함\n",
    "- 또한, 입력 데이터의 노이즈, 희소성을 줄일 수 있다고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/tokenized/ratings_mecab.txt data/tokenized/korquad_mecab.txt > data/tokenized/for-lsa-mecab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create (word, contexts) matrix\n",
      "  - counting word frequency from 276937 sents, mem=6.083 Gb\n",
      "  - scanning (word, context) pairs from 276937 sents, mem=6.083 Gb\n",
      "  - (word, context) matrix was constructed. shape = (26618, 26618)                    \n",
      "  - done\n"
     ]
    }
   ],
   "source": [
    "#단어-문맥 행렬에 LSA 적용\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from soynlp.vectorizer import sent_to_word_contexts_matrix\n",
    "\n",
    "corpus_fname = \"/notebooks/embedding/data/tokenized/for-lsa-mecab.txt\"\n",
    "\n",
    "corpus = [sent.replace(\"\\n\", \"\").strip() for sent in open(corpus_fname, \"r\").readlines()]\n",
    "input_matrix, idx2vocab = sent_to_word_contexts_matrix(\n",
    "    corpus,\n",
    "    windows=3, #타깃 단어 앞뒤 몇 개의 단어를 문맥으로 고려할지\n",
    "    min_tf=10,\n",
    "    dynamic_weight=True, #타깃 단어에서 멀어질수록 카운트 동시 등장 점수를 조금씩 깎는다.\n",
    "    verbose=True) #구축 과정을 화면에 표시할지 여부\n",
    "\n",
    "cooc_svd = TruncatedSVD(n_components=100) #100차원으로 축소\n",
    "cooc_vecs = cooc_svd.fit_transform(input_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPMI 행렬에 LSA 적용\n",
    "from soynlp.word import pmi\n",
    "ppmi_matrix, _, _ = pmi(input_matrix, min_pmi=0)\n",
    "ppmi_svd = TruncatedSVD(n_components=100)\n",
    "ppmi_vecs = ppmi_svd.fit_transform(input_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create (word, contexts) matrix\n",
      "  - counting word frequency from 276937 sents, mem=0.315 Gb\n",
      "  - scanning (word, context) pairs from 276937 sents, mem=0.959 Gb\n",
      "  - (word, context) matrix was constructed. shape = (26618, 26618)                    \n",
      "  - done\n"
     ]
    }
   ],
   "source": [
    "#LSA 학습\n",
    "!python models/word_utils.py --method latent_semantic_analysis \\\n",
    "    --input_path /notebooks/embedding/data/tokenized/for-lsa-mecab.txt \\\n",
    "    --output_path /notebooks/embedding/data/word-embeddings/lsa/lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('진실', 0.948269609730858),\n",
       " ('의식', 0.944978793251788),\n",
       " ('즐거움', 0.9364362372010614),\n",
       " ('인내심', 0.9330672039991788),\n",
       " ('사냥', 0.933050890647996)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#단어-문맥 행렬 + LSA\n",
    "from models.word_eval import WordEmbeddingEvaluator\n",
    "model = WordEmbeddingEvaluator(\"data/word-embeddings/lsa/lsa-cooc.vecs\",\n",
    "                                method=\"lsa\", dim=100, tokenizer_name=\"mecab\")\n",
    "model.most_similar(\"희망\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('진실', 0.9483008998589746),\n",
       " ('의식', 0.9450403597791779),\n",
       " ('즐거움', 0.936476402199458),\n",
       " ('사냥', 0.9333204748437434),\n",
       " ('인내심', 0.9327951308057698)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PPMI + LSA\n",
    "from models.word_eval import WordEmbeddingEvaluator\n",
    "model = WordEmbeddingEvaluator(\"data/word-embeddings/lsa/lsa-pmi.vecs\",\n",
    "                              method=\"lsa\", dim=100, tokenizer_name=\"mecab\")\n",
    "model.most_similar(\"희망\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
